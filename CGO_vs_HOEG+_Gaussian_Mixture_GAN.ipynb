{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from copy import deepcopy\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "torch.manual_seed(456)\n",
    "np.random.seed(456)\n",
    "random.seed(456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batchlen, plot=False):\n",
    "    cov = torch.tensor(np.identity(2) * 0.01, dtype=torch.float64)\n",
    "    mu1 = torch.tensor([2 ** (-1 / 2), 2 ** (-1 / 2)], dtype=torch.float64)\n",
    "    mu2 = torch.tensor([0, 1], dtype=torch.float64)\n",
    "\n",
    "    gaussian1 = MultivariateNormal(loc=mu1, covariance_matrix=cov)\n",
    "    gaussian2 = MultivariateNormal(loc=mu2, covariance_matrix=cov)\n",
    "\n",
    "    d1 = gaussian1.rsample((int(batchlen / 2),))\n",
    "    d2 = gaussian2.rsample((int(batchlen / 2),))\n",
    "\n",
    "    data = np.concatenate((d1, d2), axis=0)\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    if plot:\n",
    "        plt.scatter(data[:, 0], data[:, 1], s=2.0, color='gray')\n",
    "        plt.show()\n",
    "    return torch.Tensor(data).to(device)\n",
    "\n",
    "\n",
    "def visualise_single(real_batch, fake_batch, filename=None, fake_color='cyan'):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(5,5))\n",
    "    ax.scatter(real_batch[:, 0].cpu(), real_batch[:, 1].cpu(), s=2.0, label='real data', color='midnightblue')\n",
    "    ax.scatter(fake_batch[:, 0].cpu(), fake_batch[:, 1].cpu(), s=2.0, label='fake data', color=fake_color)\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_xlim(-1.5, 2.)\n",
    "    ax.set_ylim(-1., 2.5)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.locator_params('x', nbins=4)\n",
    "    ax.locator_params('y', nbins=4)\n",
    "    plt.show()\n",
    "    if filename is not None:\n",
    "        fig.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "    return fake_batch\n",
    "\n",
    "def visualise_all(real_batch, all_fake_batches, filename=None):\n",
    "    n_algorithms = len(all_fake_batches.keys())\n",
    "    n_graphs_per_alg = len(all_fake_batches[list(all_fake_batches.keys())[0]].keys())\n",
    "    legends = []\n",
    "\n",
    "    n_rows = n_algorithms * 2\n",
    "    n_cols = n_graphs_per_alg // 2\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols + 8, 4 * n_rows))\n",
    "\n",
    "    for i in range(n_rows):\n",
    "\n",
    "        alg_name = list(all_fake_batches.keys())[i // n_algorithms]\n",
    "\n",
    "        for j in range(n_cols):\n",
    "\n",
    "            title, data = list(all_fake_batches[alg_name].items())[(i % n_algorithms) * n_cols + j]\n",
    "\n",
    "            if alg_name == 'CGO':\n",
    "                alg_color = 'cyan'\n",
    "            elif alg_name == 'CGD':\n",
    "                alg_color = 'brown'\n",
    "            else:\n",
    "                raise NotImplemented\n",
    "\n",
    "            axes[i, j].set_aspect(aspect=1.)\n",
    "            axes[i, j].scatter(real_batch[:, 0].cpu(), real_batch[:, 1].cpu(), s=2.0, label='real data', color='midnightblue')\n",
    "            axes[i, j].scatter(data[:, 0].cpu(), data[:, 1].cpu(), s=2.0, label=alg_name, color=alg_color)\n",
    "            axes[i, j].set_xlim(-0.75, 1.25)\n",
    "            axes[i, j].set_ylim(-0.25, 1.75)\n",
    "            axes[i, j].set_title(title, fontsize=12, pad=0)\n",
    "            axes[i, j].tick_params(axis='both', which='major', labelsize=8)\n",
    "            axes[i, j].set_yticklabels([])\n",
    "            axes[i, j].set_xticklabels([])\n",
    "            axes[i, j].locator_params('x', nbins=4)\n",
    "            axes[i, j].locator_params('y', nbins=4)\n",
    "            if j == n_cols-1 and (i % n_algorithms) == 0:\n",
    "                legends.append(axes[i, j].legend(loc='upper center', bbox_to_anchor=(1.6, 1.), fancybox=True, ncol=1, prop={'size': 16}))\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    for legend in legends:\n",
    "        for handle in legend.legendHandles:\n",
    "            handle.set_sizes([40.0])\n",
    "\n",
    "    plt.show()\n",
    "    if filename is not None:\n",
    "        fig.savefig(filename, dpi=300, bbox_inches='tight', bbox_extra_artists=legends)\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, hidden_size=0, noise_size=1, noise_std=1.):\n",
    "        super().__init__()\n",
    "        self.noise_size = noise_size\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        self.fc1 = nn.Linear(noise_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.fc6 = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def __call__(self, z):\n",
    "        h = F.relu(self.fc1(z))\n",
    "\n",
    "        h = F.relu(self.fc2(h))\n",
    "        h = F.relu(self.fc3(h))\n",
    "        h = F.relu(self.fc4(h))\n",
    "        h = F.relu(self.fc5(h))\n",
    "\n",
    "        return self.fc6(h)\n",
    "\n",
    "    def generate(self, batchlen):\n",
    "        z = torch.normal(torch.zeros(batchlen, self.noise_size), self.noise_std).to(device)\n",
    "        return self.__call__(z)\n",
    "\n",
    "\n",
    "# Define the discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, hidden_size=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc6 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        return self.fc6(x)\n",
    "\n",
    "def GAN(eta = 0.05, TRAIN_RATIO=1, N_ITER=250, BATCHLEN=128, hidden_size_G=0, hidden_size_D=0, noise_size=1, noise_std=1., frame=1000, alpha = 0.1, L= 100, delta = 0.05, Lip = 100, mu =0.5, show=True):\n",
    "    \"\"\"\n",
    "    TRAIN_RATIO : int, number of times to train the discriminator between two generator steps\n",
    "    N_ITER : int, total number of training iterations for the generator\n",
    "    BATCHLEN : int, Batch size to use\n",
    "    hidden_size_G : int, width of the generator (number of neurons in hidden layers)\n",
    "    hidden_size_D : int, width of the discriminator (number of neurons in hidden layers)\n",
    "    noise_size : int, dimension of input noise\n",
    "    noise_std : float, standard deviation of p(z)\n",
    "    frame : int, display data each 'frame' iteration\n",
    "    \"\"\"\n",
    "    fixed_batch = generate_batch(1024)  # for visualisation purposes\n",
    "    all_fake_batches = OrderedDict()\n",
    "    alg = \"HOEG+\"\n",
    "    all_fake_batches[alg] = OrderedDict()\n",
    "    \n",
    "    G = Generator(hidden_size=hidden_size_G, noise_size=noise_size, noise_std=noise_std).to(device)\n",
    "    D = Discriminator(hidden_size=hidden_size_D).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    G_optimiser = torch.optim.SGD(G.parameters(), lr=1)\n",
    "    D_optimiser = torch.optim.SGD(D.parameters(), lr=1)\n",
    "    \n",
    "    \n",
    "    for i in tqdm(range(N_ITER+1)):\n",
    "        # visualization\n",
    "        if i % frame == 0:\n",
    "            print(f\"Iteration {i}\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                all_fake_batches[alg][f'Iteration {i}'] = G.generate(1024)\n",
    "\n",
    "            if show:\n",
    "                visualise_single(real_batch=fixed_batch, fake_batch=all_fake_batches[alg][f'Iteration {i}'],\n",
    "                                 fake_color='cyan' if alg == \"CGO\" else \"brown\")\n",
    "\n",
    "        # train the discriminator\n",
    "        real_batch = generate_batch(BATCHLEN)\n",
    "        fake_batch = G.generate(BATCHLEN).to(device)\n",
    "\n",
    "        # Compute here the total loss\n",
    "        h_real = D(real_batch)\n",
    "        h_fake = D(fake_batch)\n",
    "\n",
    "        loss_real = criterion(h_real, torch.ones((BATCHLEN, 1)).to(device))\n",
    "        loss_fake = criterion(h_fake, torch.zeros((BATCHLEN, 1)).to(device))\n",
    "\n",
    "        total_loss = loss_real + loss_fake\n",
    "\n",
    "        # Compute the higher order update for both agents\n",
    "        D_update, G_update = compute_hoegp_update(f=total_loss,\n",
    "                                            g=-total_loss,\n",
    "                                            x=list(D.parameters()),\n",
    "                                            y=list(G.parameters()),delta = delta, mu = mu)  # real learning rate\n",
    "        \n",
    "        lambda_k = 0\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Apply the update\n",
    "            for p, update in zip(D.parameters(), D_update):\n",
    "                lambda_k += torch.norm(update)**2\n",
    "                p.grad = update\n",
    "\n",
    "            for p, update in zip(G.parameters(), G_update):\n",
    "                lambda_k += torch.norm(update)**2\n",
    "                p.grad = update\n",
    "\n",
    "        lambda_k = lambda_k**(-0.5)\n",
    "#         print('lambda',lambda_k)\n",
    "        G_optimiser.step()\n",
    "        D_optimiser.step()\n",
    "        ## 1st order update\n",
    "        # train the discriminator\n",
    "\n",
    "        real_batch = generate_batch(BATCHLEN)\n",
    "        fake_batch = G.generate(BATCHLEN).to(device)\n",
    "\n",
    "        # Compute here the total loss\n",
    "        h_real = D(real_batch)\n",
    "        h_fake = D(fake_batch)\n",
    "\n",
    "        loss_real = criterion(h_real, torch.ones((BATCHLEN, 1)).to(device))\n",
    "        loss_fake = criterion(h_fake, torch.zeros((BATCHLEN, 1)).to(device))\n",
    "\n",
    "        total_loss = loss_real + loss_fake\n",
    "\n",
    "        # Obtain the first-order gradients\n",
    "        df_dx = grad(outputs=total_loss, inputs=list(D.parameters()), create_graph=True, retain_graph=True)\n",
    "        dg_dy = grad(outputs=-total_loss, inputs=list(G.parameters()), create_graph=True, retain_graph=True)\n",
    "\n",
    "        D_update_1, G_update_1 = df_dx, dg_dy\n",
    "        # freeze optimizer\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Apply the update\n",
    "            for p, z_i, update in zip(D.parameters(), D_update, D_update_1):\n",
    "                p.grad = lambda_k*update/Lip-z_i\n",
    "\n",
    "            for p, z_i, update in zip(G.parameters(), G_update, G_update_1):\n",
    "                p.grad = lambda_k*update/Lip-z_i\n",
    "\n",
    "        G_optimiser.step()\n",
    "        D_optimiser.step()\n",
    "    plt.plot(list_norm)\n",
    "    plt.show()\n",
    "#     visualise_all(real_batch=fixed_batch, all_fake_batches=all_fake_batches, filename=\"Experiment2_GAN_GM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOEG+ updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_norm(x):\n",
    "    temp = 0\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        temp += torch.norm(x[i])**2\n",
    "        \n",
    "    return(temp)\n",
    "\n",
    "def compute_hoegp_update(f,g,x,y,delta,mu):\n",
    "    #set parameters\n",
    "    \n",
    "    start = time.time()       \n",
    "    l = 0\n",
    "    \n",
    "    df_dx = grad(outputs=f, inputs=x, create_graph=True, retain_graph=True)\n",
    "    dg_dy = grad(outputs=g, inputs=y, create_graph=True, retain_graph=True)\n",
    "     \n",
    "    with torch.no_grad():\n",
    "        norm_F = (tuple_norm(df_dx)+tuple_norm(dg_dy))**0.5\n",
    "        \n",
    "    u = norm_F/delta\n",
    "    nu = delta*mu**2/norm_F\n",
    "    \n",
    "    lambd = (l+u)/2\n",
    "    lambda_m = lambd-nu\n",
    "    list_norm.append(norm_F)\n",
    "#     print(norm_F,len(list_norm))\n",
    "    #1st order gradients\n",
    "    x_update_1, y_update_1 = conjugate_gradient(lambd,f,x,g,y)\n",
    "    temp_1 = (tuple_norm(x_update_1)+tuple_norm(y_update_1))**0.5\n",
    "    x_update, y_update = conjugate_gradient(lambda_m,f,x,g,y)\n",
    "    temp_2 = (tuple_norm(x_update)+tuple_norm(y_update))**0.5 \n",
    "\n",
    "    i = 0\n",
    "    while not ((temp_1 <= lambd) and (temp_2> lambda_m)):\n",
    "#         print(i)\n",
    "#         print(temp_1,lambd,temp_2,lambda_m)\n",
    "        if lambd <= delta*mu**2/norm_F:\n",
    "            break\n",
    "                            \n",
    "        if temp_1 <= lambd:\n",
    "            u = lambd\n",
    "            lambd = (l+u)/2\n",
    "            lambda_m = lambd-nu\n",
    "        else:\n",
    "            l = lambd\n",
    "            lambd = (l+u)/2\n",
    "            lambda_m = lambd-nu\n",
    "\n",
    "        x_update_1, y_update_1 = conjugate_gradient(lambd,f,x,g,y)\n",
    "        temp_1 = (tuple_norm(x_update_1)+tuple_norm(y_update_1))**0.5\n",
    "        x_update, y_update = conjugate_gradient(lambda_m,f,x,g,y)\n",
    "        temp_2 = (tuple_norm(x_update)+tuple_norm(y_update))**0.5 \n",
    "        i += 1\n",
    "    return(x_update_1, y_update_1)    \n",
    "\n",
    "\n",
    "def conjugate_gradient(lambd, f, x, g, y, max_it=10):\n",
    "    \"\"\"\n",
    "    Iteratively estimate the solution for the local Nash equilibrium using the conjugate gradient method\n",
    "    f: loss function to minimise for player X\n",
    "    x: current action (parameters) of player X\n",
    "    g: loss function to minimise for player Y\n",
    "    y: current action (parameters) of player y\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    # Computing the gradients\n",
    "\n",
    "    df_dx = grad(outputs=f, inputs=x, create_graph=True, retain_graph=True)\n",
    "    dg_dy = grad(outputs=g, inputs=y, create_graph=True, retain_graph=True)\n",
    "\n",
    "    df_dy = grad(outputs=f, inputs=y, create_graph=True, retain_graph=True)\n",
    "    dg_dx = grad(outputs=g, inputs=x, create_graph=True, retain_graph=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Creating the appropriate structure for the parameter updates and initialising to 0\n",
    "\n",
    "        x_update, y_update = [], []\n",
    "        for x_grad_group, y_grad_group in zip(df_dx, dg_dy):\n",
    "            x_update.append(torch.zeros_like(x_grad_group))\n",
    "            y_update.append(torch.zeros_like(y_grad_group))\n",
    "\n",
    "        # Creating the appropriate structure for the residuals and basis vectors and initialise them\n",
    "\n",
    "        r_xk, r_yk, p_xk, p_yk = [], [], [], []\n",
    "        for x_param_update, y_param_update in zip(df_dx, dg_dy):\n",
    "            r_xk.append(torch.clone(x_param_update))\n",
    "            p_xk.append(torch.clone(x_param_update))\n",
    "            r_yk.append(torch.clone(y_param_update))\n",
    "            p_yk.append(torch.clone(y_param_update))\n",
    "\n",
    "    # Iteratively solve for the local Nash Equilibrium\n",
    "\n",
    "    for k in range(max_it):\n",
    "\n",
    "        # Computes the Hessian-vector product Ap\n",
    "\n",
    "        hvp_x = grad(outputs=df_dy, inputs=x, grad_outputs=p_yk, retain_graph=True)\n",
    "        hvp_y = grad(outputs=dg_dx, inputs=y, grad_outputs=p_xk, retain_graph=True)\n",
    "\n",
    "        pure_x = grad(outputs=df_dx, inputs=x, grad_outputs=p_xk, retain_graph=True)\n",
    "        pure_y = grad(outputs=dg_dy, inputs=y, grad_outputs=p_yk, retain_graph=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Computes the matrix-basisVector product Ap\n",
    "\n",
    "            Ap_x, Ap_y = [], []\n",
    "            for i in range(len(p_xk)):\n",
    "                Ap_x.append(pure_x[i] + hvp_x[i] + lambd*p_xk[i])\n",
    "                Ap_y.append(pure_y[i] + hvp_y[i] + lambd*p_yk[i])\n",
    "\n",
    "            # Computes step size alpha_k\n",
    "\n",
    "            num, denom = 0., 0.\n",
    "            for i in range(len(r_xk)):\n",
    "\n",
    "                r_k_i = torch.cat([r_xk[i].flatten(), r_yk[i].flatten()])\n",
    "                num += torch.dot(r_k_i, r_k_i)\n",
    "\n",
    "                Ap_i = torch.cat([Ap_x[i].flatten(), Ap_y[i].flatten()])\n",
    "                p_k_i = torch.cat([p_xk[i].flatten(), p_yk[i].flatten()])\n",
    "\n",
    "                denom += torch.dot(p_k_i, Ap_i)\n",
    "\n",
    "            alpha_k = num / denom\n",
    "\n",
    "            # Computes new updates\n",
    "\n",
    "            for i in range(len(x_update)):\n",
    "                x_update[i] += alpha_k * p_xk[i]\n",
    "                y_update[i] += alpha_k * p_yk[i]\n",
    "\n",
    "            # Computes new residuals\n",
    "\n",
    "            r_xkplus1, r_ykplus1 = [], []\n",
    "            for i in range(len(r_xk)):\n",
    "                r_xkplus1.append(r_xk[i] - alpha_k * Ap_x[i])\n",
    "                r_ykplus1.append(r_yk[i] - alpha_k * Ap_y[i])\n",
    "\n",
    "            # Check convergence condition\n",
    "\n",
    "            r_xkplus1_squared_sum, r_ykplus1_squared_sum = 0., 0.\n",
    "            x_update_squared_norm, y_update_squared_norm = 0., 0.\n",
    "            for i in range(len(r_xkplus1)):\n",
    "                r_xkplus1_squared_sum += torch.sum(r_xkplus1[i] ** 2.)\n",
    "                r_ykplus1_squared_sum += torch.sum(r_ykplus1[i] ** 2.)\n",
    "\n",
    "                x_update_squared_norm += torch.sum(x_update[i] ** 2.)\n",
    "                y_update_squared_norm += torch.sum(y_update[i] ** 2.)\n",
    "\n",
    "            r_kplus1_norm = torch.sqrt(r_xkplus1_squared_sum + r_ykplus1_squared_sum)\n",
    "            update_norm = torch.sqrt(x_update_squared_norm + y_update_squared_norm)\n",
    "\n",
    "            if r_kplus1_norm <= 1e-6:\n",
    "                break\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Computes beta_k\n",
    "\n",
    "                num, denom = 0., 0.\n",
    "                for i in range(len(r_xk)):\n",
    "                    r_kplus1_i = torch.cat([r_xkplus1[i].flatten(), r_ykplus1[i].flatten()])\n",
    "                    denom += torch.dot(r_kplus1_i, r_kplus1_i)\n",
    "\n",
    "                    r_k_i = torch.cat([r_xk[i].flatten(), r_yk[i].flatten()])\n",
    "                    denom += torch.dot(r_k_i, r_k_i)\n",
    "\n",
    "                beta_k = num / denom\n",
    "\n",
    "                # Computes new basis vectors\n",
    "\n",
    "                for i in range(len(p_xk)):\n",
    "                    p_xk[i] = r_xkplus1[i] + beta_k * p_xk[i]\n",
    "                    p_yk[i] = r_ykplus1[i] + beta_k * p_yk[i]\n",
    "\n",
    "                r_xk = deepcopy(r_xkplus1)\n",
    "                r_yk = deepcopy(r_ykplus1)\n",
    "                \n",
    "    return x_update, y_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c79194a560642aab1220706cfa5ccaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2501), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch = generate_batch(256, plot=False)\n",
    "list_norm = []\n",
    "\n",
    "GAN(TRAIN_RATIO=2,\n",
    "    N_ITER=2500,\n",
    "    BATCHLEN=1024,\n",
    "    hidden_size_G=128,\n",
    "    hidden_size_D=128,\n",
    "    noise_size=512,\n",
    "    noise_std=6,\n",
    "    frame=25,    \n",
    "    delta = 0.1,\n",
    "    Lip = 100,\n",
    "    mu = 0.1,\n",
    "    show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = generate_batch(256, plot=False)\n",
    "list_norm = []\n",
    "\n",
    "GAN(TRAIN_RATIO=2,\n",
    "    N_ITER=2500,\n",
    "    BATCHLEN=1024,\n",
    "    hidden_size_G=128,\n",
    "    hidden_size_D=128,\n",
    "    noise_size=512,\n",
    "    noise_std=6,\n",
    "    frame=25,    \n",
    "    delta = 0.05,\n",
    "    Lip = 1000,\n",
    "    mu = 0.1,\n",
    "    show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = generate_batch(256, plot=False)\n",
    "list_norm = []\n",
    "\n",
    "GAN(TRAIN_RATIO=2,\n",
    "    N_ITER=2500,\n",
    "    BATCHLEN=1024,\n",
    "    hidden_size_G=128,\n",
    "    hidden_size_D=128,\n",
    "    noise_size=512,\n",
    "    noise_std=6,\n",
    "    frame=25,    \n",
    "    delta = 0.01,\n",
    "    Lip = 1000,\n",
    "    mu = 0.1,\n",
    "    show=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
