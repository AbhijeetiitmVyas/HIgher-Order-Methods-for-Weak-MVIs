# -*- coding: utf-8 -*-
"""Best GAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ig3slOeQkRjJagb1HyKmud0gHzp947sV
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import grad
from torch.distributions.multivariate_normal import MultivariateNormal
from copy import deepcopy
import time
from collections import OrderedDict
from tqdm import tqdm_notebook as tqdm
import matplotlib.pyplot as plt
import random

import seaborn as sns
sns.set()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

torch.manual_seed(456)
np.random.seed(456)
random.seed(456)

"""# Utils"""

def generate_batch(batchlen, plot=False):
    cov = torch.tensor(np.identity(2) * 0.01, dtype=torch.float64)
    mu1 = torch.tensor([2 ** (-1 / 2), 2 ** (-1 / 2)], dtype=torch.float64)
    mu2 = torch.tensor([0, 1], dtype=torch.float64)

    gaussian1 = MultivariateNormal(loc=mu1, covariance_matrix=cov)
    gaussian2 = MultivariateNormal(loc=mu2, covariance_matrix=cov)

    d1 = gaussian1.rsample((int(batchlen / 2),))
    d2 = gaussian2.rsample((int(batchlen / 2),))

    data = np.concatenate((d1, d2), axis=0)
    np.random.shuffle(data)

    if plot:
        plt.scatter(data[:, 0], data[:, 1], s=2.0, color='gray')
        plt.show()
    return torch.Tensor(data).to(device)


def visualise_single(real_batch, fake_batch, filename=None, fake_color='cyan'):
    fig, ax = plt.subplots(1,1, figsize=(5,5))
    ax.scatter(real_batch[:, 0].cpu(), real_batch[:, 1].cpu(), s=2.0, label='real data', color='midnightblue')
    ax.scatter(fake_batch[:, 0].cpu(), fake_batch[:, 1].cpu(), s=2.0, label='fake data', color=fake_color)
    ax.legend(loc='upper left')
    ax.set_xlim(-1.5, 2.)
    ax.set_ylim(-1., 2.5)
    ax.tick_params(axis='both', which='major', labelsize=12)
    ax.locator_params('x', nbins=4)
    ax.locator_params('y', nbins=4)
    plt.show()
    if filename is not None:
        fig.savefig(filename, dpi=300, bbox_inches='tight')

    plt.close(fig)

    return fake_batch

def visualise_all(real_batch, all_fake_batches, filename=None):
    n_algorithms = len(all_fake_batches.keys())
    n_graphs_per_alg = len(all_fake_batches[list(all_fake_batches.keys())[0]].keys())
    legends = []

    n_rows = n_algorithms * 2
    n_cols = n_graphs_per_alg // 2
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols + 8, 4 * n_rows))

    for i in range(n_rows):

        alg_name = list(all_fake_batches.keys())[i // n_algorithms]

        for j in range(n_cols):

            title, data = list(all_fake_batches[alg_name].items())[(i % n_algorithms) * n_cols + j]

            if alg_name == 'CGO':
                alg_color = 'cyan'
            elif alg_name == 'CGD':
                alg_color = 'brown'
            else:
                raise NotImplemented

            axes[i, j].set_aspect(aspect=1.)
            axes[i, j].scatter(real_batch[:, 0].cpu(), real_batch[:, 1].cpu(), s=2.0, label='real data', color='midnightblue')
            axes[i, j].scatter(data[:, 0].cpu(), data[:, 1].cpu(), s=2.0, label=alg_name, color=alg_color)
            axes[i, j].set_xlim(-0.75, 1.25)
            axes[i, j].set_ylim(-0.25, 1.75)
            axes[i, j].set_title(title, fontsize=12, pad=0)
            axes[i, j].tick_params(axis='both', which='major', labelsize=8)
            axes[i, j].set_yticklabels([])
            axes[i, j].set_xticklabels([])
            axes[i, j].locator_params('x', nbins=4)
            axes[i, j].locator_params('y', nbins=4)
            if j == n_cols-1 and (i % n_algorithms) == 0:
                legends.append(axes[i, j].legend(loc='upper center', bbox_to_anchor=(1.6, 1.), fancybox=True, ncol=1, prop={'size': 16}))

    fig.tight_layout()

    for legend in legends:
        for handle in legend.legendHandles:
            handle.set_sizes([40.0])

    plt.show()
    if filename is not None:
        fig.savefig(filename, dpi=300, bbox_inches='tight', bbox_extra_artists=legends)

    plt.close(fig)

"""# Models"""

# Define the generator
class Generator(nn.Module):
    def __init__(self, hidden_size=0, noise_size=1, noise_std=1.):
        super().__init__()
        self.noise_size = noise_size
        self.noise_std = noise_std

        self.fc1 = nn.Linear(noise_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, hidden_size)
        self.fc4 = nn.Linear(hidden_size, hidden_size)
        self.fc5 = nn.Linear(hidden_size, hidden_size)

        self.fc6 = nn.Linear(hidden_size, 2)

    def __call__(self, z):
        h = F.relu(self.fc1(z))

        h = F.relu(self.fc2(h))
        h = F.relu(self.fc3(h))
        h = F.relu(self.fc4(h))
        h = F.relu(self.fc5(h))

        return self.fc6(h)

    def generate(self, batchlen):
        z = torch.normal(torch.zeros(batchlen, self.noise_size), self.noise_std).to(device)
        return self.__call__(z)


# Define the discriminator
class Discriminator(nn.Module):
    def __init__(self, hidden_size=0):
        super().__init__()

        self.fc1 = nn.Linear(2, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, hidden_size)
        self.fc4 = nn.Linear(hidden_size, hidden_size)
        self.fc5 = nn.Linear(hidden_size, hidden_size)
        self.fc6 = nn.Linear(hidden_size, 1)

    def __call__(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = F.relu(self.fc4(x))
        x = F.relu(self.fc5(x))
        return self.fc6(x)

def GAN(eta = 0.05, TRAIN_RATIO=1, N_ITER=250, BATCHLEN=128, hidden_size_G=0, hidden_size_D=0, noise_size=1, noise_std=1., frame=1000, alpha = 0.1, L= 100, delta = 0.05, Lip = 100, mu =0.5, show=True):
    """
    TRAIN_RATIO : int, number of times to train the discriminator between two generator steps
    N_ITER : int, total number of training iterations for the generator
    BATCHLEN : int, Batch size to use
    hidden_size_G : int, width of the generator (number of neurons in hidden layers)
    hidden_size_D : int, width of the discriminator (number of neurons in hidden layers)
    noise_size : int, dimension of input noise
    noise_std : float, standard deviation of p(z)
    frame : int, display data each 'frame' iteration
    """
    fixed_batch = generate_batch(1024)  # for visualisation purposes
    all_fake_batches = OrderedDict()
    alg = "HOEG+"
    all_fake_batches[alg] = OrderedDict()

    G = Generator(hidden_size=hidden_size_G, noise_size=noise_size, noise_std=noise_std).to(device)
    D = Discriminator(hidden_size=hidden_size_D).to(device)

    criterion = nn.BCEWithLogitsLoss()

    G_optimiser = torch.optim.SGD(G.parameters(), lr=1)
    D_optimiser = torch.optim.SGD(D.parameters(), lr=1)


    for i in tqdm(range(N_ITER+1)):
        # visualization
        if i % frame == 0:
            print(f"Iteration {i}")

            with torch.no_grad():
                all_fake_batches[alg][f'Iteration {i}'] = G.generate(1024)

            if show:
                visualise_single(real_batch=fixed_batch, fake_batch=all_fake_batches[alg][f'Iteration {i}'],
                                 fake_color='cyan' if alg == "CGO" else "brown")
            plt.plot(list_norm)
            plt.show()
        # train the discriminator
        real_batch = generate_batch(BATCHLEN)
        fake_batch = G.generate(BATCHLEN).to(device)

        # Compute here the total loss
        h_real = D(real_batch)
        h_fake = D(fake_batch)

        loss_real = criterion(h_real, torch.ones((BATCHLEN, 1)).to(device))
        loss_fake = criterion(h_fake, torch.zeros((BATCHLEN, 1)).to(device))

        total_loss = loss_real + loss_fake

        # Compute the higher order update for both agents
        D_update, G_update = compute_hoegp_update(f=total_loss,
                                            g=-total_loss,
                                            x=list(D.parameters()),
                                            y=list(G.parameters()),delta = delta, mu = mu)  # real learning rate

        lambda_k = 0
        with torch.no_grad():

            # Apply the update
            for p, update in zip(D.parameters(), D_update):
                lambda_k += torch.norm(update)**2
                p.grad = update

            for p, update in zip(G.parameters(), G_update):
                lambda_k += torch.norm(update)**2
                p.grad = update

        lambda_k = lambda_k**(-0.5)
#         print('lambda',lambda_k)
        G_optimiser.step()
        D_optimiser.step()
        ## 1st order update
        # train the discriminator

        real_batch = generate_batch(BATCHLEN)
        fake_batch = G.generate(BATCHLEN).to(device)

        # Compute here the total loss
        h_real = D(real_batch)
        h_fake = D(fake_batch)

        loss_real = criterion(h_real, torch.ones((BATCHLEN, 1)).to(device))
        loss_fake = criterion(h_fake, torch.zeros((BATCHLEN, 1)).to(device))

        total_loss = loss_real + loss_fake

        # Obtain the first-order gradients
        df_dx = grad(outputs=total_loss, inputs=list(D.parameters()), create_graph=True, retain_graph=True)
        dg_dy = grad(outputs=-total_loss, inputs=list(G.parameters()), create_graph=True, retain_graph=True)

        D_update_1, G_update_1 = df_dx, dg_dy
        # freeze optimizer
        with torch.no_grad():

            # Apply the update
            for p, z_i, update in zip(D.parameters(), D_update, D_update_1):
                p.grad = lambda_k*update/Lip-z_i

            for p, z_i, update in zip(G.parameters(), G_update, G_update_1):
                p.grad = lambda_k*update/Lip-z_i

        G_optimiser.step()
        D_optimiser.step()

#     visualise_all(real_batch=fixed_batch, all_fake_batches=all_fake_batches, filename="Experiment2_GAN_GM")

"""# HOEG+ updates"""

def tuple_norm(x):
    temp = 0

    for i in range(len(x)):
        temp += torch.norm(x[i])**2

    return(temp)

def compute_hoegp_update(f,g,x,y,delta,mu):
    #set parameters

    start = time.time()
    l = 0

    df_dx = grad(outputs=f, inputs=x, create_graph=True, retain_graph=True)
    dg_dy = grad(outputs=g, inputs=y, create_graph=True, retain_graph=True)

    with torch.no_grad():
        norm_F = (tuple_norm(df_dx)+tuple_norm(dg_dy))**0.5

    u = norm_F/delta
    nu = delta*mu**2/norm_F

    lambd = (l+u)/2
    lambda_m = lambd-nu
    list_norm.append(norm_F.item())

    x_update_1, y_update_1 = conjugate_gradient(lambd,f,x,g,y)
    temp_1 = (tuple_norm(x_update_1)+tuple_norm(y_update_1))**0.5
    x_update, y_update = conjugate_gradient(lambda_m,f,x,g,y)
    temp_2 = (tuple_norm(x_update)+tuple_norm(y_update))**0.5

    i = 0
    while not ((temp_1 <= lambd) and (temp_2> lambda_m)):

        if lambd <= delta*mu**2/norm_F:
            break

        if temp_1 <= lambd:
            u = lambd
            lambd = (l+u)/2
            lambda_m = lambd-nu
        else:
            l = lambd
            lambd = (l+u)/2
            lambda_m = lambd-nu

        x_update_1, y_update_1 = conjugate_gradient(lambd,f,x,g,y)
        temp_1 = (tuple_norm(x_update_1)+tuple_norm(y_update_1))**0.5
        x_update, y_update = conjugate_gradient(lambda_m,f,x,g,y)
        temp_2 = (tuple_norm(x_update)+tuple_norm(y_update))**0.5
        i += 1
    return(x_update_1, y_update_1)


def conjugate_gradient(lambd, f, x, g, y, max_it=10):
    """
    Iteratively estimate the solution for the local Nash equilibrium using the conjugate gradient method
    f: loss function to minimise for player X
    x: current action (parameters) of player X
    g: loss function to minimise for player Y
    y: current action (parameters) of player y
    """
    start = time.time()

    # Computing the gradients

    df_dx = grad(outputs=f, inputs=x, create_graph=True, retain_graph=True)
    dg_dy = grad(outputs=g, inputs=y, create_graph=True, retain_graph=True)

    df_dy = grad(outputs=f, inputs=y, create_graph=True, retain_graph=True)
    dg_dx = grad(outputs=g, inputs=x, create_graph=True, retain_graph=True)

    with torch.no_grad():

        # Creating the appropriate structure for the parameter updates and initialising to 0

        x_update, y_update = [], []
        for x_grad_group, y_grad_group in zip(df_dx, dg_dy):
            x_update.append(torch.zeros_like(x_grad_group))
            y_update.append(torch.zeros_like(y_grad_group))

        # Creating the appropriate structure for the residuals and basis vectors and initialise them

        r_xk, r_yk, p_xk, p_yk = [], [], [], []
        for x_param_update, y_param_update in zip(df_dx, dg_dy):
            r_xk.append(torch.clone(x_param_update))
            p_xk.append(torch.clone(x_param_update))
            r_yk.append(torch.clone(y_param_update))
            p_yk.append(torch.clone(y_param_update))

    # Iteratively solve for the local Nash Equilibrium

    for k in range(max_it):

        # Computes the Hessian-vector product Ap

        hvp_x = grad(outputs=df_dy, inputs=x, grad_outputs=p_yk, retain_graph=True)
        hvp_y = grad(outputs=dg_dx, inputs=y, grad_outputs=p_xk, retain_graph=True)

        pure_x = grad(outputs=df_dx, inputs=x, grad_outputs=p_xk, retain_graph=True)
        pure_y = grad(outputs=dg_dy, inputs=y, grad_outputs=p_yk, retain_graph=True)

        with torch.no_grad():

            # Computes the matrix-basisVector product Ap

            Ap_x, Ap_y = [], []
            for i in range(len(p_xk)):
                Ap_x.append(pure_x[i] + hvp_x[i] + lambd*p_xk[i])
                Ap_y.append(pure_y[i] + hvp_y[i] + lambd*p_yk[i])

            # Computes step size alpha_k

            num, denom = 0., 0.
            for i in range(len(r_xk)):

                r_k_i = torch.cat([r_xk[i].flatten(), r_yk[i].flatten()])
                num += torch.dot(r_k_i, r_k_i)

                Ap_i = torch.cat([Ap_x[i].flatten(), Ap_y[i].flatten()])
                p_k_i = torch.cat([p_xk[i].flatten(), p_yk[i].flatten()])

                denom += torch.dot(p_k_i, Ap_i)

            alpha_k = num / denom

            # Computes new updates

            for i in range(len(x_update)):
                x_update[i] += alpha_k * p_xk[i]
                y_update[i] += alpha_k * p_yk[i]

            # Computes new residuals

            r_xkplus1, r_ykplus1 = [], []
            for i in range(len(r_xk)):
                r_xkplus1.append(r_xk[i] - alpha_k * Ap_x[i])
                r_ykplus1.append(r_yk[i] - alpha_k * Ap_y[i])

            # Check convergence condition

            r_xkplus1_squared_sum, r_ykplus1_squared_sum = 0., 0.
            x_update_squared_norm, y_update_squared_norm = 0., 0.
            for i in range(len(r_xkplus1)):
                r_xkplus1_squared_sum += torch.sum(r_xkplus1[i] ** 2.)
                r_ykplus1_squared_sum += torch.sum(r_ykplus1[i] ** 2.)

                x_update_squared_norm += torch.sum(x_update[i] ** 2.)
                y_update_squared_norm += torch.sum(y_update[i] ** 2.)

            r_kplus1_norm = torch.sqrt(r_xkplus1_squared_sum + r_ykplus1_squared_sum)
            update_norm = torch.sqrt(x_update_squared_norm + y_update_squared_norm)

            if r_kplus1_norm <= 1e-6:
                break

            else:

                # Computes beta_k

                num, denom = 0., 0.
                for i in range(len(r_xk)):
                    r_kplus1_i = torch.cat([r_xkplus1[i].flatten(), r_ykplus1[i].flatten()])
                    denom += torch.dot(r_kplus1_i, r_kplus1_i)

                    r_k_i = torch.cat([r_xk[i].flatten(), r_yk[i].flatten()])
                    denom += torch.dot(r_k_i, r_k_i)

                beta_k = num / denom

                # Computes new basis vectors

                for i in range(len(p_xk)):
                    p_xk[i] = r_xkplus1[i] + beta_k * p_xk[i]
                    p_yk[i] = r_ykplus1[i] + beta_k * p_yk[i]

                r_xk = deepcopy(r_xkplus1)
                r_yk = deepcopy(r_ykplus1)

    return x_update, y_update

"""# Main()"""

batch = generate_batch(256, plot=False)
list_norm = []

GAN(TRAIN_RATIO=2,
    N_ITER=2500,
    BATCHLEN=128,
    hidden_size_G=128,
    hidden_size_D=128,
    noise_size=512,
    noise_std=6,
    frame=25,
    delta = 0.1,
    Lip = 10,
    mu = 0.5,
    show=True)



batch = generate_batch(256, plot=False)
list_norm = []

GAN(TRAIN_RATIO=2,
    N_ITER=2500,
    BATCHLEN=1024,
    hidden_size_G=128,
    hidden_size_D=128,
    noise_size=512,
    noise_std=6,
    frame=25,
    delta = 0.05,
    Lip = 10,
    mu = 0.1,
    show=True)

batch = generate_batch(256, plot=False)
list_norm = []

GAN(TRAIN_RATIO=2,
    N_ITER=2500,
    BATCHLEN=1024,
    hidden_size_G=128,
    hidden_size_D=128,
    noise_size=512,
    noise_std=6,
    frame=25,
    delta = 0.01,
    Lip = 10,
    mu = 0.1,
    show=True)

batch = generate_batch(256, plot=False)
list_norm = []

GAN(TRAIN_RATIO=2,
    N_ITER=2500,
    BATCHLEN=1024,
    hidden_size_G=128,
    hidden_size_D=128,
    noise_size=512,
    noise_std=6,
    frame=25,
    delta = 0.05,
    Lip = 100,
    mu = 0.1,
    show=True)

batch = generate_batch(256, plot=False)
list_norm = []

GAN(TRAIN_RATIO=2,
    N_ITER=2500,
    BATCHLEN=1024,
    hidden_size_G=128,
    hidden_size_D=128,
    noise_size=512,
    noise_std=6,
    frame=25,
    delta = 0.01,
    Lip = 100,
    mu = 0.1,
    show=True)
